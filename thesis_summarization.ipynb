{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "840a3c6b",
   "metadata": {},
   "source": [
    "OpenAPI를 이용한 논문, 기사, 글 요약 및 번역"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543b6b04",
   "metadata": {},
   "source": [
    "API 키 발급\n",
    " - RapidAPI, 네이버"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fed4c6",
   "metadata": {},
   "source": [
    "TLDRThis\n",
    "1. Abstractive(Human-like) summarization\n",
    "    - Abstractive summarization(생성 요약)은 기존 Input text를 그대로 인용하지 않고,     기존의 내용을 새롭게 re-pharasing 하여 Summary를 생성하는 요약모델\n",
    "2. Extractive summarization\n",
    "    - 반면에 Extractive summarization(추출 요약)은 기존 Input text에 존재하는 중요한       단어를 그대로 사용하여 Summary를 생성하는 요약 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87d97f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb37a7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': ['The goal of this paper is to learn strong lip reading models that can recognise speech in silent videos. Most prior works deal with the open-set visual speech recognition problem by adapting existing automatic speech recognition techniques on top of trivially pooled visual features. Instead, in this paper, we focus on the unique challenges encountered in lip reading and propose tailored solutions. We propose an attention-based pooling mechanism to aggregate visual speech representations. We use sub-word units for lip reading for the first time and show that this allows us to better model the ambiguities of the task. Finally, we propose a model for Visual Speech Detection (VSD), trained ontop of the lip reading network.'], 'article_text': \"The goal of this paper is to learn strong lip reading models that can recognise speech in silent videos. Most prior works deal with the open-set visual speech recognition problem by adapting existing automatic speech recognition techniques on top of trivially pooled visual features. Instead, in this paper, we focus on the unique challenges encountered in lip reading and propose tailored solutions. To this end, we make the following contributions: (1) we propose an attention-based pooling mechanism to aggregate visual speech representations; (2) we use sub-word units for lip reading for the first time and show that this allows us to better model the ambiguities of the task; (3) we propose a model for Visual Speech Detection (VSD), trained on top of the lip reading network. Following the above, we obtain state-of-the-art results on the challenging LRS2 and LRS3 benchmarks when training on public datasets, and even surpass models trained on large-scale industrial datasets by using an order of magnitude less data. Our best model achieves 22.6% word error rate on the LRS2 dataset, a performance unprecedented for lip reading models, significantly reducing the performance gap between lip reading and automatic speech recognition. Moreover, on the AVA-ActiveSpeaker benchmark, our VSD model surpasses all visual-only baselines and even outperforms several recent audio-visual methods. Introduction Lip reading, or visual speech recognition, is the task of recognising speech from silent video. It has many practical applications which include improving speech recognition in noisy environments, enabling silent dictation, or dubbing and transcribing archival silent films [25] . It also has important medical applications, such as helping speech impaired individuals, e.g. people suffering from Lou Gehrig's disease speak [54] , or enabling people with aphonia (loss of voice) to communicate just by using lip movements. Lip reading and audio-based automatic speech recog-nition (ASR) both have the common goal of transcribing speech, however, they differ regarding the input: while in ASR the input signal is an audio waveform, in essence, a one-dimensional time series, lip reading has to deal with high-dimensional video inputs that have both temporal and spatial complexity. This makes training large end-to-end models harder due to GPU memory and computation constraints. Furthermore, understanding speech from visual information alone is challenging due to the inherent ambiguities present in the visual stream, i.e. the existence of homophemes where different characters that are visually indistinguishable (e.g. 'pa', 'ba', and 'ma'). That lip reading is a much harder task is also supported by the fact that although humans can understand speech reasonably well even in the presence of noise and across a variety of accents, they perform relatively poorly on lip reading [7, 16] . Designing a lip reading model requires both a visual component -mouth movements need to be identified -as well as a temporal sequence modelling component, which typically involves learning a language model that can resolve ambiguities in individual lip shapes. Recent developments in deep learning models and the availability of largescale annotated datasets has led to breakthroughs surpassing human performance [16] . However, most of these works have taken the approach of adapting techniques used for ASR and machine translation, without catering to the particularities of the vision problem. The conjecture in this paper is that the performance of lip reading, in terms of both accuracy and data efficiency, can be improved if the model is designed from the start taking into account the peculiarities of the visual, rather than the audio domain. To this end, we consider both the visual encoding and the text tokenisation. Visual encoding. Our first contribution is the design of a novel visual backbone for lip reading. The spatiotemporal complexity in lip reading requires dealing with problems such as tracking the mouth in moving talking heads. This is usually achieved with complicated pre-processing pipelines based on facial landmarks. However, those are sub-optimal in many cases. For example, landmarks don't work well in profile views [26] . Moreover, it is unclear what is the optimal region-of-interest for lip reading: it has been shown that besides the lips, other parts of the face, e.g. the cheeks, may also contain useful discriminative information [68] . Also, this region-of-interest can vary drastically in terms of scale, aspect ratio across identities and utterances. Thus, in this work, we propose an end-to-end trainable attention-based pooling mechanism that learns to track and aggregate the lip movement representations, resulting in a significant performance boost. Text tokenisation. Lip reading methods most commonly output character-level tokens. This output representation however is sub-optimal as characters are sometimes more fine-grained than the input, with multiple characters corresponding to a single video frame. Furthermore, characters do not encode any prior knowledge about the language, which leads to higher dependency on the decoder's language modeling capacity that must also 'learn to read'. In this work, we instead use sub-word tokens (word-pieces) which not only match with multiple adjacent frames but are also semantically meaningful for learning a language easily. Word-pieces result in much shorter (than character) output sequences which greatly reduces the run-time and memory requirements. They also provide a language prior, reducing the language modelling burden of the model. We experimentally compare character and word-piece tokenization to justify this choice. Visual Speech Detection. One issue with performing lip reading inference on real-world silent videos is that, since there is no audio track, there is no automated procedure for cropping out the clips where the person is speaking. ASR models use Voice Activity Detection (VAD) as a key preprocessing step, but this is clearly not applicable for silent videos. Here, the parts of a video containing speech have to be determined using the video input alone; in other words, by performing Visual Speech Detection (VSD). This can be very useful e.g. for running inference on silent movies. Among other findings in this work, we show that it is possible to train a strong VSD model on top of our pre-trained lip reading encoder. Other downstream tasks. Besides improving performance on the sentence-level lip reading task itself, obtaining improved lip movement representations can have a broader impact, as those are often used for other related downstream tasks -e.g. sound source separation [19] , visual keyword spotting [42] , and visual language identification [4] . In summary, we make the following three contributions: (i) a visual backbone architecture using attentionbased pooling on the spatial feature map; (ii) the use of subword units, rather than characters for the language tokens; and (iii) a strong Visual Speech Detection model, directly trained on top of the lip reading encoder. In the experiments we show the benefits of (i) and (ii) on improving lip reading performance, and we also introduce a two-stage training protocol that simplifies the curriculum used in prior works. As will be seen, with these design choices and training methodology, the performance of our best models exceeds prior work on standard evaluation benchmarks, and even outperforms proprietary models that use an order of magnitude more data for training. Similarly, we show the benefit of (i) and the lip reading encoder to our visual speech detection model that is far superior to previous methods on a standard evaluation benchmark. We discuss potential ethical concerns and limitations of our work in the arXiv version. Please check our project page for video examples, code, and pre-trained models. Related Work We present an overview of prior work on lip reading, including a discussion of how these methods select and track the visual regions of interest, as well as the output tokenizations they use, followed by a brief overview of the use of attention for visual feature aggregation in other domains. Lip reading. Early works on lip reading relied on handcrafted pipelines and statistical models for visual feature extraction and temporal modelling [21, 37, 43, 44, 48] ; an extensive review of those methods is presented in [70] . The advent of deep learning and the availability of large-scale lip reading datasets such as LRS2 [15] and LRS3 [2] , rejuvenated this area. Progress was initially on word-level recognition [16, 58] , and then moved onto sentence-level recognition by adapting models developed for ASR using LSTM sequence-to-sequence [15] or CTC [7, 54] approaches. [47] take a hybrid approach, training an LSTM-based sequenceto-sequence model with an auxiliary CTC loss. One trend in recent work is moving to Transformer-based architectures [1] , or variants using convolution blocks [67] , and hybrid architectures like a Conformer [22] . Another trend is to investigate the benefits of training with larger datasets, either directly by training on proprietary data that is orders of magnitude larger than any public dataset [40] , or indirectly by distilling ASR models into lip reading ones [3, 33, 65] . For visual feature extraction and short-term dynamic modelling, most modern pipelines rely on spatiotemporal CNNs consisting of multiple 3D convolutional layers [7, 54] , or more lightweight alternatives that comprise a single 3D convolutional layer followed by 2D ones [1, 16, 58] applied frame-wise. Mouth ROI selection, registration, and tracking. A thorough investigation on facial region of interest (ROI) selection for lip reading is provided by [68] . The videos included in datasets like LRS2 and LRS3 are commonly preprocessed with a face detection and tracking pipeline which outputs clips roughly centered around the speaker's face. Many previous works use a central crop on the provided videos as input to the feature extractors [1, 38, 58] . More elaborate pipelines use facial landmarks to register the face to a canonical view and/or only extract the crops of the mouth area [7, 31, 40, 46, 54, 67] . [68] propose to input a large part of the face, combined with Cutout [17] to encourage the model to also use the extra-oral face regions. After selecting which input region to extract the low-level CNN features from, all above works apply Global Average Pooling (GAP) on the extracted visual features map; this obtains a compact representation, but discards spatial information. Recent works [67] have shown that replacing GAP with a spatio-temporal fusion module improves performance. Text tokenization. Most prior works on lip reading output character-level predictions [1, 14, 15, 38, 40, 47, 67] . Those approaches usually use an external language model during inference to boost performance [27, 39] . Instead [54] chose to output phoneme sequences, using phonetic dictionaries. This approach has the advantage of a more accurate mapping of lip-movements to sounds but requires a complicated decoding pipeline involving a proprietary finite-state-transducer. [20, 30] use a hard-crafted heuristic to map words onto viseme sequences and vice versa, and use viseme tokens for representing the output and target text. In this work, we instead propose using sub-word level tokenisation, which greatly reduces the output sequence length, thus accelerating both training and inference, and neatly encodes prior language information that improves the overall performance. Visual feature aggregation with attention. Our work is also related to methods that use attention for improving visual representations of images or videos. [24, 61] use attention-weighted-averages of visual features as building blocks for various classification and detection tasks, while OCNet [66] uses self-attention to model context between pixels for semantic segmentation. Several recent papers have replaced convolutions with Transformer [60] blocks in visual representation pipelines. DETR [11] and Efficient DETR [64] learn object detectors by applying spatial transformers on top of CNN feature extractors. Similarly, the Visual Transformer [62] tokenises low-level CNN features and then processes them using a Transformer to model relationships between tokens. ViT [18] completely replaces CNNs in the visual pipeline with Transformer layers applied on image patch sequences, while the Timesformer [10] has been suggested as a purely Transformer-based solution for video representation learning. Speech Detection. An important pre-processing stage in ASR pipelines is Voice Activity Detection (VAD), which involves the detection of the presence of speech in audio [51] . The reliability of audio-based VAD systems deteriorates in the presence of noise or cocktail party scenarios [35] . In audio-visual pipelines, such as ones used for creating large-scale audio-visual speech datasets [2, 16] , this step is commonly replaced by an Active Speaker Detection (ASD) stage which determines face tracks that match the speech. Audio-visual ASD models have been effectively trained either using direct supervision [6, 13, 32, 52, 59] or in a self-supervised [5, 16] fashion by employing contrastive objectives. The visual counterpart to VAD is Visual Speech Detection (VAD), which operates only on the video input. Early work on VSD (also termed Visual VAD or V-VAD) was based on handcrafted visual features and statistical modelling using methods such as HMMs, GMMs and PCA [8, 34-36, 45, 49, 55-57] . More recent works proposed methods based on optical flow [9] or a combination of CNN and LSTMs [23, 53] . These methods are limited in having been trained or evaluated on constrained or non-public datasets. The train set of WildVVAD [23] , a new annotated VSD dataset has been made public, however, at the time of publication of this paper, its test set was not available, we were, therefore, unable to use this dataset for benchmarking. Method In this section, we describe our proposed method. The architecture of the model is outlined in Figure 1 . Next, we explain each stage of the pipeline and refer the reader to the arXiv version for further details. Visual backbone CNN. The input to the pipeline is a silent video clip of T frames, x ∈ R T ×H×W ×3 . A spatio-temporal residual CNN is applied on sub-clips of 5 frames (i.e. 0.2s) with a unit frame stride, to extract visual spatial feature maps f ∈ R T ×h×w×c . For our best model, H = W = 96, (h, w) = (H/4, W/4) = (24, 24), and c = 128. Visual Transformer Pooling (VTP). The CNN feature map f t ∈ R hw×c corresponding to every input frame t ∈ {1, . . . , T } is processed individually by a shared Visual Transformer Pooling (VTP) block. The feature map is first flattened to f t ∈ R h×w×c and projected to a desired Transformer feature dimension d to get f t ∈ R hw×d . Then, spatial positional encodings (SPE) are added to it; the result is passed through an encoder consisting of N V T P Transformer layers, to get an enhanced self-attended feature map z t = encoder v (f t + SP E 1:hw ) ∈ R hw×d . A learnable query vector Q att ∈ R d×1 is then used to extract a visual attention mask a t = sof tmax(Q att z t ) ∈ R hw×1 . The attention mask is used to compute a weighted average over the self-attended feature map to produce a self-attended feature map zt. A query vector Qatt is used to compute an attention mask which is in turn used to obtain a spatially weighted average of zt. This produces a compact visual representation of the lip appearance and movement around each input video frame. Concatenating the frame-wise features forms a temporal feature sequence g. This is passed as input to an encoder-decoder Transformer (right) that auto-regressively predicts sub-word probabilities for one token at a time. An output sentence is eventually inferred from these distributions using a beam search. g t = 1 hw hw u=1 a u t z u t ∈ R d where a u t and z u t denote the feature and attention weight respectively, associated with frame t and location u ∈ {1, . . . , hw}. By stacking the resulting vectors g t in time, we obtain an embedding sequence g = (g 1 , g 2 • • • , g T ) ∈ R T ×d which contains a compact spatio-temporal representation for every input frame. Transformer encoder-decoder. An encoder-decoder Transformer model is used to predict a text token sequence s = (s 1 , s 2 • • • , s T dec ) from the source video embedding sequence g, one token at a time: temporal positional encodings (PE) are added to g, and the result is input to an encoder, which consists of N enc multi-head Transformer layers, to produce a self-attended embedding sequence g enc = ENCODER(g + P E 1:T ) ∈ R T ×d . The decoder, which consists of N dec Transformer layers, then attends on this sequence and predicts the output text token sequence s in an auto-regressive manner, by factorising its joint probability: log p(s|x) = T dec t=1 log p(s t | g enc (x), s 1:t−1 ) (1) where positional encodings have also been added to the auto-regressive decoder inputs as in [60] . The text sentences are encoded into token sequences (and vice versa tokens are decoded into text) using a subword level tokeniser, in particular WordPiece [63] . We tried other sub-word tokenizations such as Byte-Pair-Encoding (BPE) that is used in GPT2 [50] , but it performed worse compared to using WordPiece. Beam search decoding and rescoring. Decoding is performed with a left-to-right beam search of width B. We also decode a second time after flipping all the input video frames horizontally. Additional language knowledge is incorporated by using an external language model (LM) to re-score [12] the 2 × B-best hypotheses S = {s 1 • • • s B ; s h 1 • • • s h B } from the beam searches, and obtain the highest scoring one as the final sentence prediction: s best = arg max s∈S [ α log p(s|x) + (1 − α) log p LM (s) ] Here, s h 1 • • • s h B denotes the beam sequences after horizontally flipping the input. We found that additional testtime augmentations such as small degrees of rotation and/or color jitters did not improve the results. Training Optimisation objective. Given a training dataset D consisting of pairs (x, s * ) of video clips and their ground truth transcriptions, the model is trained to maximise the log likelihoods of the transcriptions by optimising the following objective L = −E (x,s * )∈D log p(s * |x) (2) Teacher forcing. To accelerate training, we follow common practice for sequence-to-sequence training with Transformers, and feed in the previous ground-truth token as the decoder input at every step, instead of using autoregression. The tokens are fed into the decoder via a learnable embedding layer. Training protocol. Training is performed in two stages. First, the whole network is trained end-to-end on short phrases of 2 words. Following [1, 16] , we use frame wordboundaries to crop out training samples from all the possible combinations of 2 consecutive words in the dataset, which provides natural augmentation. Upon convergence, we freeze the visual backbone, then pre-extract and dump the visual features for all the samples. In the second training stage that follows, we train the encoder-decoder subnetwork on all possible sub-sequences (n-grams) of length 2 or larger that can be generated by combining consecutive word utterances in the dataset. Discussion. We note that our training protocol is much simpler than the ones commonly used in prior works [1, 16, 47] . By using the same network and loss during the backbone pre-training stage, we obtain a good initialization of the entire network and enable a smooth transfer. This is in contrast to other works that pre-train with a different proxy loss and require a separate word classification head that is subsequently discarded. The second stage is significantly simpler to implement and requires a single run, unlike curriculums that gradually increase the length of the training sentences and usually require a complicated tuning process with multiple manual restarts to achieve the best results. We observed that our proposed second stage training setup matches the performance of the complicated curriculum strategy used in previous works, while being more efficient in terms of training time and manual efforts. Implementation details During the first training stage, we apply random visual augmentations on the input frames to reduce overfitting: the input videos are first resized to a square 160 pixels resolution, from which a central square 96-pixel crop is extracted. Random horizontal flipping and rotation (up to 10 • ) are also applied before inputting to the lip reading pipeline. During inference we use the central 96-pixel crop, and only apply the horizontal flipping augmentation. For our best model, i.e., VTP on (H/4, W/4), we set N V T P = 6 layers with 8 heads each for the encoder of the VTP module. For computational efficiency, VTP uses the recently proposed Linear Transformer [28] instead of the original Transformer [60] . We found that this change did not lead to a drop in the recognition performance, while being much more computationally efficient. Another design choice we had to make is deciding after which CNN layer the VTP should be applied. Transformer layers are computationally expensive at higher resolution feature maps (i.e. earlier layer activations), but can capture more detailed information. Given this trade-off, we experiment with three different feature map resolutions, at (h, w) = (H/4, W/4), (H/8, W/8), (H/16, W/16). For the latter two variants, we set the feature dimension d = 512. When pooling on (h, w) = (H/4, W/4), we keep the compute and memory needs in check by performing two small changes: using d = 256 for the first 3 VTP layers, and then setting d = 512 but down-sampling the feature map to (H/8, W/8) for the remaining 3 layers. The encoder-decoder Transformer contains N enc = 6 and N dec = 6 layers, each with 8 attention heads. We use sinusoidal positional encodings [60] [40] , even though we are only using an order of magnitude less data. This is indicative of the data efficiency of our proposed pipeline. † Large non-public labelled datasets: MV-LRS [1] contains 730 hours, LSVSR [54] 3.9k hours, and YT31k [40] 31k hours of transcribed video. ‡ unlabelled dataset. Results shown in blue have been obtained by training (partly or entirely) on non-public data. enizer of the BERT model in HuggingFace 1 , with a vocabulary of 30522 tokens. We also use an off-the-shelf pretrained GPT2 language model for beam rescoring. For the beam rescoring, we set hyperparameter α = 0.7 for LRS2 and α = 0.6 for LRS3 respectively. We train all models with the Adam optimiser [29] with β 1 = 0.9, β 2 = 0.98 and = 10 −9 . In the first stage of the training, we follow a Noam learning rate schedule [60] for the first 50 epochs and then reduce the learning rate by a factor of 5 every time the validation loss plateaus, until reaching 10 −6 . For the second stage, the learning rate is initially set to 5e −5 and reduced by a factor of 5 on plateau down to 10 −6 . For our best reported models on public data, the first stage of training takes approximately 14 days on 4 Tesla v100s GPUs. The second stage takes 1.5 days on 1 Tesla v100 GPU. State-of-the-art lipreading We compare the results of our method to existing works in Table 1 . It is clear that our best model outperforms all prior work trained on public data, on both the LRS2 and LRS3 benchmarks. In particular, compared to the strongest baseline of [38] , our best model performs 9% better on LRS2 and 2.7% better on LRS3. When also using MV-LRS and TEDx ext for training, we obtain a significant boost, achieving 22.6% and 30.7% WER for LRS2 and LRS3 respectively. We even outperform [40] by a considerable mar-1 https://huggingface.co/transformers/pretrained models.html gin while using 10× lesser training data. This clearly suggests that our pipeline is highly data-efficient. Ablations We perform various ablations to better understand the different aspects of our pipeline. For all the ablation experiments the models are trained and evaluated on public data only, i.e. the LRS2 and LRS3 datasets. Importance of each module. We show the impact of each proposed module in the final scores, starting from a variation of the TM-seq2seq model [1] 2 , and building up to our full model. We summarize the results of this study in Table 2. It is clear that all the proposed improvements give significant performance boosts and are largely orthogonal. In particular, the use of WordPiece tokens contributes a 3.8% absolute improvement on LRS2, while introducing the VTP module decreases the WER by 6.3%. Using LM to rescore the beams and applying test-time horizontal flipping leads to another 1.1% and 0.9% improvement respectively. VTP resolution. The VTP module is capable of aggregating the spatial features at arbitrary feature map resolutions. But, we show that it is more effective when operating on finer high-resolution feature maps rather than coarser lowresolution feature maps. This is evident in Table 3 , where pooling after conv 2,3 at a spatial resolution of 24 × 24 is much more effective than pooling on lower-resolution feature maps of 12 × 12 or 6 × 6. Visual attention visualization In Figure 2 , we visualize the visual attention maps that the VTP module produces. Note that the lips region is tracked very accurately while the speakers turn their heads around, even for extreme profile views. Visual Speech Detection Application We build a VSD model on top of our lip reading transformer encoder by simply adding a fully connected (FC) layer and a sigmoid activation on top of the frame-level encoder outputs to classify whether the person is speaking in that frame or not: y = σ( FC (g enc ) ) ∈ R T . The architecture is illustrated in Figure 3 . We train the VSD head on top of the pre-trained lip reading encoder using a binary cross-entropy loss between the predicted and ground truth labels, y t and y * t : L v = 1 T T t=1 y * t log y t + (1 − y * t ) log(1 − y t ) (3) Dataset and evaluation. We train our VSD model on the train split of the popular AVA ActiveSpeaker dataset [52] . This dataset is created from movies and contains 120 videos (2.6M frames) for training, 33 videos (768K frames) for validation and 109 videos (2M frames) in the test set. Each frame contains bounding box annotations for the face, along with a label indicating if the person is (i) speaking and audible, (ii) speaking but not audible, (iii) inaudible. The second class covers cases where the person is visually speaking in the background, but his voice/speech is not audible. Since we operate only on the visual frames, we combine the samples of the first two classes and train the model to perform binary classification. We initialize the weights from our pretrained lip-reading models and fine-tune all the layers with the Adam optimizer by using a small learning rate of 10 −6 . To evaluate the performance of our model and baselines we use the mean Average Precision (mAP) metric as defined by the dataset authors [52] ; we compute the metric using the evaluation script 3 that the authors supply. We also report our scores on the held-out non-public test set with the assistance of the Ava-ActiveSpeaker challenge organizers. Results. We show quantitative results in [52] by a large margin (over 17 mAP improvement). In fact, we even outperform several of the recently proposed audio-visual methods ( [6, 13] ), obtaining results close to the current state-of-the-art without using any audio. Limitations and Ethical considerations. We explore the limitations and failure cases of the lip reading and the VSD models in the arXiv version. We also discuss the ethical issues and the positive real-world applications of our work. Conclusion We have presented an improved architecture for lip reading based on attention-based aggregation of visual representations as well as several enhancements to the training protocol, including the use of sub-word tokenisation. Our best models achieve state-of-the-art results, outperforming prior work trained on public data by a significant margin, and even industrial models trained on orders of magnitude more data. We have also designed a Visual Speech Detection model on top of our lip reading system that obtains state-of-the-art results on this task and even outperforms several audio-visual baselines. Acknowledgements. Funding for this research is provided by the EPSRC Programme Grant VisualAI EP/T028572/1, and a DeepMind Graduate Scholarship.\", 'article_title': 'Sub-word Level Lip Reading With Visual Attention', 'article_authors': ['K R Prajwal', 'Triantafyllos  Afouras', 'Andrew  Zisserman'], 'article_image': None, 'article_pub_date': '', 'article_url': 'https://openaccess.thecvf.com/content/CVPR2022/papers/Prajwal_Sub-Word_Level_Lip_Reading_With_Visual_Attention_CVPR_2022_paper.pdf', 'article_html': None, 'article_abstract': 'The goal of this paper is to learn strong lip reading models that can recognise speech in silent videos. Most prior works deal with the open-set visual speech recognition problem by adapting existing automatic speech recognition techniques on top of trivially pooled visual features. Instead, in this paper, we focus on the unique challenges encountered in lip reading and propose tailored solutions. To this end, we make the following contributions: (1) we propose an attention-based pooling mechanism to aggregate visual speech representations; (2) we use sub-word units for lip reading for the first time and show that this allows us to better model the ambiguities of the task; (3) we propose a model for Visual Speech Detection (VSD), trained on top of the lip reading network. Following the above, we obtain state-of-the-art results on the challenging LRS2 and LRS3 benchmarks when training on public datasets, and even surpass models trained on large-scale industrial datasets by using an order of magnitude less data. Our best model achieves 22.6% word error rate on the LRS2 dataset, a performance unprecedented for lip reading models, significantly reducing the performance gap between lip reading and automatic speech recognition. Moreover, on the AVA-ActiveSpeaker benchmark, our VSD model surpasses all visual-only baselines and even outperforms several recent audio-visual methods.'}\n"
     ]
    }
   ],
   "source": [
    "url = \"https://tldrthis.p.rapidapi.com/v1/model/abstractive/summarize-url/\"\n",
    "\n",
    "payload = {\n",
    "\t\"url\": \"https://openaccess.thecvf.com/content/CVPR2022/papers/Prajwal_Sub-Word_Level_Lip_Reading_With_Visual_Attention_CVPR_2022_paper.pdf\",\n",
    "\t\"min_length\": 100,\n",
    "\t\"max_length\": 300,\n",
    "\t\"is_detailed\": False\n",
    "}\n",
    "headers = {\n",
    "\t\"content-type\": \"application/json\",\n",
    "\t\"X-RapidAPI-Key\": rapidapi_key,\n",
    "\t\"X-RapidAPI-Host\": \"tldrthis.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, json=payload, headers=headers)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "404f9840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully Automatic License Plate Recognition (ALPR) has been a frequent research topic due to several practical applications. The paper presents a robust and efficient ALPR system based on the state-of-the-art YOLO object detector and Normalizing flows. We demonstrate that our proposed model can learn on a small number of samples free of single or multiple characters. Multi-scale image transformations are implemented to provide a solution to the problem of the problem. The dataset will also be made publicly available to encourage further studies and research on plate detection.\n"
     ]
    }
   ],
   "source": [
    "summary = response.json()['summary'][0].strip()\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cd3c15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': {'result': {'srcLangType': 'en', 'tarLangType': 'ko', 'translatedText': '완전 자동 번호판 인식(ALPR)은 여러 가지 실용적인 응용으로 인해 자주 연구 주제가 되어 왔다. 이 논문은 최첨단 YOLO 객체 감지기와 정규화 흐름을 기반으로 강력하고 효율적인 ALPR 시스템을 제시한다. 제안된 모델이 단일 또는 여러 문자 없이 적은 수의 샘플에서 학습할 수 있음을 보여준다. 다중 스케일 이미지 변환은 문제의 해결책을 제공하기 위해 구현된다. 또한 데이터 세트는 플레이트 탐지에 대한 추가 연구와 연구를 장려하기 위해 공개적으로 제공될 것이다.', 'engineType': 'UNDEF_MULTI_SENTENCE', 'pivot': None, 'dict': None, 'tarDict': None}, '@type': 'response', '@service': 'naverservice.nmt.proxy', '@version': '1.0.0'}}\n"
     ]
    }
   ],
   "source": [
    "url = \"https://openapi.naver.com/v1/papago/n2mt\"\n",
    "\n",
    "payload ={\n",
    "    \"source\" : \"en\",\n",
    "    \"target\" : \"ko\",\n",
    "    \"text\" : summary,\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"content-type\" : \"application/json\",\n",
    "    \"X-Naver-Client-Id\" : naver_client_id,\n",
    "    \"X-Naver-Client-Secret\" : naver_client_secret,\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, json=payload, headers=headers)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4546610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완전 자동 번호판 인식(ALPR)은 여러 가지 실용적인 응용으로 인해 자주 연구 주제가 되어 왔다. 이 논문은 최첨단 YOLO 객체 감지기와 정규화 흐름을 기반으로 강력하고 효율적인 ALPR 시스템을 제시한다. 제안된 모델이 단일 또는 여러 문자 없이 적은 수의 샘플에서 학습할 수 있음을 보여준다. 다중 스케일 이미지 변환은 문제의 해결책을 제공하기 위해 구현된다. 또한 데이터 세트는 플레이트 탐지에 대한 추가 연구와 연구를 장려하기 위해 공개적으로 제공될 것이다.\n"
     ]
    }
   ],
   "source": [
    "print(response.json()['message']['result']['translatedText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1db68101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_and_translate(article_url, min_length=100, max_length=300):\n",
    "    url = \"https://tldrthis.p.rapidapi.com/v1/model/abstractive/summarize-url/\"\n",
    "\n",
    "    payload = {\n",
    "        \"url\": article_url,\n",
    "        \"min_length\": min_length,\n",
    "        \"max_length\": max_length,\n",
    "        \"is_detailed\": False\n",
    "    }\n",
    "    headers = {\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"X-RapidAPI-Key\": rapidapi_key,\n",
    "        \"X-RapidAPI-Host\": \"tldrthis.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, json=payload, headers=headers)\n",
    "\n",
    "    summary = response.json()['summary'][0].strip()\n",
    "\n",
    "    url = \"https://openapi.naver.com/v1/papago/n2mt\"\n",
    "\n",
    "    payload ={\n",
    "        \"source\" : \"en\",\n",
    "        \"target\" : \"ko\",\n",
    "        \"text\" : summary,\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"content-type\" : \"application/json\",\n",
    "        \"X-Naver-Client-Id\" : naver_client_id,\n",
    "        \"X-Naver-Client-Secret\" : naver_client_secret,\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, json=payload, headers=headers)\n",
    "\n",
    "    return response.json()['message']['result']['translatedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a610f21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'다중 객체 추적(MOT)은 비디오 스트림에서 여러 객체의 공간-시간 궤적을 감지하고 추정하는 것을 목표로 한다. Bot-Sort-ReID와 우리의 더 간단한 버전 BoSort는 리드보드의 다른 모든 추적기로부터 최고의 IDF1, MOTA 및 HOTA 성능을 달성한다. 우리는 카메라 모션 보상 기반 추적기 및 적절한 필터 최첨단 기능과 같은 개선 사항을 추가하여 탐지별 추적기를 크게 개선할 수 있음을 보여준다.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_and_translate(\"https://arxiv.org/pdf/2206.14651v2.pdf\", 100, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76c4287b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YOU LOW ONCE(YOLO) 알고리즘에 대한 자세한 내용은 이 조사 설문에서 설명합니다. 이 조사는 실시간 객체 감지 방향의 YOLO와 CNN(Convolution Neural Network)에 관한 것이다. CNN 아키텍처 모델은 하이라이트를 제거하고 주어진 이미지에서 객체를 식별할 수 있는 기능을 가지고 있다. CNN 모델은 변형 진단, 교육적 또는 유익한 응용 프로그램 생성 등과 같은 문제를 해결할 수 있다. 이 논문은 분석을 통해 관찰 횟수와 관점 발견에 도달했다. 또한 금융 및 기타 산업에서 집중적인 시각적 정보와 기능 추출을 지원합니다.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_and_translate(\"https://arxiv.org/ftp/arxiv/papers/2208/2208.00773.pdf\", 100, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dddb0c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SuperYOLO는 멀티모달 데이터를 융합하고 멀티스케일 객체에 대해 고해상도(HR) 객체 감지를 수행한다. 모델은 최첨단 모델에 비해 유리한 정확도-속도 절충을 보여준다. 물체 감지는 컴퓨터 지원 진단 또는 자율 조종과 같은 다양한 분야에서 중요한 역할을 한다. 코드는 https:////github.com/icey-zhang/SuperYOLo에서 공개됩니다. RSI를 위한 정확하고 빠른 소형 물체 감지 방법을 제안한다.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_and_translate(\"https://arxiv.org/pdf/2209.13351.pdf\", 100, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f8f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
